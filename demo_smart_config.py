#!/usr/bin/env python3
"""
üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Any2Any Trainer

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞–∫ —Ç–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –ø—Ä–æ—Å—Ç–æ —É–∫–∞–∑–∞—Ç—å –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏
–∏ —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏!
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), "src"))

from any2any_trainer.utils.config import TrainingConfig
import yaml


def demo_smart_configs():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É–º–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
    print("üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —É–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Any2Any Trainer\n")
    
    # 1. –ü—Ä–æ—Å—Ç–∞—è LLM (text ‚Üí text)
    print("1Ô∏è‚É£ –ü—Ä–æ—Å—Ç–∞—è LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    simple_config = {
        "model_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
        "modalities": {"input": ["text"], "output": ["text"]},
        "dataset": ["HuggingFaceH4/ultrachat_200k"],
        "output_dir": "./output/simple_llm",
        "use_peft": True,
        "lora": {"r": 32, "alpha": 64}
    }
    
    config = TrainingConfig(**simple_config)
    print(f"   üìã –ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: {config.modalities}")
    print(f"   ü§ñ –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω: model_type = '{config.model_type}'\n")
    
    # 2. –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (image + text ‚Üí text)
    print("2Ô∏è‚É£ –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (LLaVA-style):")
    multimodal_config = {
        "model_name_or_path": "microsoft/DialoGPT-medium",
        "modalities": {"input": ["image", "text"], "output": ["text"]},
        "encoders": {
            "image": {"model": "openai/clip-vit-large-patch14", "freeze": True}
        },
        "dataset": ["llava_conversations.jsonl"],
        "output_dir": "./output/multimodal_model"
    }
    
    config = TrainingConfig(**multimodal_config)
    print(f"   üìã –ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: {config.modalities}")
    print(f"   ü§ñ –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω: model_type = '{config.model_type}'\n")
    
    # 3. Any2Any –º–æ–¥–µ–ª—å (–≤—Å–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏)
    print("3Ô∏è‚É£ Any2Any –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    any2any_config = {
        "model_name_or_path": "Qwen/Qwen2.5-7B-Instruct",
        "modalities": {
            "input": ["text", "image", "audio"], 
            "output": ["text", "image", "audio"]
        },
        "encoders": {
            "image": {"model": "openai/clip-vit-large-patch14", "freeze": True},
            "audio": {"model": "openai/whisper-base", "freeze": True}
        },
        "decoders": {
            "image": {"model": "stabilityai/stable-diffusion-2-1"},
            "audio": {"model": "microsoft/speecht5_tts"}
        },
        "dataset": ["custom/multimodal_conversations"],
        "output_dir": "./output/any2any_model"
    }
    
    config = TrainingConfig(**any2any_config)
    print(f"   üìã –ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: {config.modalities}")
    print(f"   ü§ñ –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω: model_type = '{config.model_type}'\n")


def show_yaml_examples():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π."""
    print("üìù –ü—Ä–∏–º–µ—Ä—ã YAML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π:\n")
    
    print("1Ô∏è‚É£ –ü—Ä–æ—Å—Ç–∞—è LLM (configs/sft/simple_modality_flow.yaml):")
    simple_yaml = """
model_name_or_path: "Qwen/Qwen2.5-1.5B-Instruct"
# model_type –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—Å—è –∫–∞–∫ "standard"!
modalities:
  input: ["text"]
  output: ["text"]
dataset: ["HuggingFaceH4/ultrachat_200k"]
use_peft: true
lora:
  r: 32
  alpha: 64
    """
    print(simple_yaml)
    
    print("2Ô∏è‚É£ –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (configs/sft/multimodal_flow.yaml):")
    multimodal_yaml = """
model_name_or_path: "microsoft/DialoGPT-medium"
# model_type –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—Å—è –∫–∞–∫ "multimodal"!
modalities:
  input: ["image", "text"]
  output: ["text"]
encoders:
  image:
    model: "openai/clip-vit-large-patch14"
    freeze: true
dataset: ["llava_conversations.jsonl"]
    """
    print(multimodal_yaml)
    
    print("3Ô∏è‚É£ Any2Any –º–æ–¥–µ–ª—å (configs/sft/any2any_flow.yaml):")
    any2any_yaml = """
model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
# model_type –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—Å—è –∫–∞–∫ "any2any"!
modalities:
  input: ["text", "image", "audio"]
  output: ["text", "image", "audio"]
encoders:
  image:
    model: "openai/clip-vit-large-patch14"
    freeze: true
  audio:
    model: "openai/whisper-base"
    freeze: true
decoders:
  image:
    model: "stabilityai/stable-diffusion-2-1"
  audio:
    model: "microsoft/speecht5_tts"
    """
    print(any2any_yaml)


def show_auto_detection_logic():
    """–û–±—ä—è—Å–Ω—è–µ—Ç –ª–æ–≥–∏–∫—É –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è."""
    print("üß† –õ–æ–≥–∏–∫–∞ –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è model_type:\n")
    
    rules = [
        ("üî§ text ‚Üí text", "standard", "–û–±—ã—á–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å"),
        ("üñºÔ∏è image ‚Üí image", "standard", "–û–¥–Ω–æ—Ç–∏–ø–Ω–∞—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å"),
        ("üñºÔ∏èüìù image+text ‚Üí text", "multimodal", "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –≤—Ö–æ–¥, —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤—ã—Ö–æ–¥"),
        ("üñºÔ∏è image ‚Üí text", "multimodal", "Vision-to-text –º–æ–¥–µ–ª—å"),
        ("üìùüñºÔ∏èüîä text+image+audio ‚Üí text", "multimodal", "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—Ö–æ–¥, —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤—ã—Ö–æ–¥"),
        ("üìù text ‚Üí text+image", "any2any", "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –≤—ã—Ö–æ–¥"),
        ("üñºÔ∏èüîä image+audio ‚Üí text+audio", "any2any", "–°–ª–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏")
    ]
    
    for modalities, model_type, description in rules:
        print(f"   {modalities:<25} ‚Üí {model_type:<12} | {description}")
    
    print()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    print("=" * 80)
    print("üéØ Any2Any Trainer - –£–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
    print("=" * 80)
    print()
    
    show_auto_detection_logic()
    demo_smart_configs()
    show_yaml_examples()
    
    print("‚ú® –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —É–º–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
    print("   ‚Ä¢ –ù–µ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –∏ —É–∫–∞–∑—ã–≤–∞—Ç—å model_type")
    print("   ‚Ä¢ –ú–µ–Ω—å—à–µ –æ—à–∏–±–æ–∫ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    print("   ‚Ä¢ –ü—Ä–æ—Å—Ç–æ–π –∏ –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å")
    print("   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è")
    print()
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è:")
    print("   poetry run python scripts/train_multimodal.py configs/sft/simple_modality_flow.yaml")
    print("   poetry run python scripts/train_multimodal.py configs/sft/multimodal_flow.yaml")
    print("   poetry run python scripts/train_multimodal.py configs/sft/any2any_flow.yaml")
    print()
    
    print("=" * 80)


if __name__ == "__main__":
    main() 