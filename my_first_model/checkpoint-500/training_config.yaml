model_name_or_path: gpt2
model_type: standard
modalities:
  input:
  - text
  output:
  - text
encoders: {}
decoders: {}
projection:
  type: mlp
  hidden_size: 4096
  num_layers: 2
  dropout: 0.1
special_tokens: {}
dataset:
- wikitext
- wikitext-2-raw-v1
conversation_field: conversations
max_seq_length: 2048
output_dir: ./my_first_model
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
num_train_epochs: 1
learning_rate: 5.0e-05
warmup_steps: 0
logging_steps: 50
save_steps: 500
eval_steps: 500
save_total_limit: 3
use_peft: true
lora:
  r: 8
  alpha: 16
  dropout: 0.1
  target_modules:
  - c_attn
  bias: none
freeze_vision_encoder: true
freeze_audio_encoder: true
freeze_llm: false
train_projection_only: false
unfreeze_layers_patterns: []
gradient_checkpointing: false
bf16: true
fp16: false
dataloader_num_workers: 4
remove_unused_columns: false
report_to: none
run_name: null
generate_eval_examples: false
max_new_tokens: 256
