# üöÄ –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º model_type
# –¢–µ–ø–µ—Ä—å –Ω–µ –Ω—É–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å model_type - –æ–Ω –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!

# === –û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
model_name_or_path: "Qwen/Qwen2.5-1.5B-Instruct"

# –ü—Ä–æ—Å—Ç–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç model_type: standard
modalities:
  input: ["text"]
  output: ["text"]

# === –î–ê–¢–ê–°–ï–¢ ===
dataset:
  - "HuggingFaceH4/ultrachat_200k"

conversation_field: "messages"
max_seq_length: 2048

# === –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ===
output_dir: "./output/simple_text_model"
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 2
num_train_epochs: 1
learning_rate: 5e-5
warmup_steps: 100
logging_steps: 10
save_steps: 500

# === LoRA ===
use_peft: true
lora:
  r: 32
  alpha: 64
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"

# === –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ===
bf16: true
dataloader_num_workers: 4
remove_unused_columns: false
report_to: "none"
run_name: "simple_text_auto"
seed: 42 