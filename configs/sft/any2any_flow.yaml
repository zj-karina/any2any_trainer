# üîÑ Any2Any –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º model_type
# –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ

# === –û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"

# –£–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç model_type: any2any
modalities:
  input: ["text", "image", "audio"]
  output: ["text", "image", "audio"]

# === –≠–ù–ö–û–î–ï–†–´ –î–õ–Ø –í–°–ï–• –ú–û–î–ê–õ–¨–ù–û–°–¢–ï–ô ===
encoders:
  image:
    model: "openai/clip-vit-large-patch14"
    freeze: true
    tokenizer_type: "discrete"
  audio:
    model: "openai/whisper-base"
    freeze: true
    tokenizer_type: "discrete"

# === –î–ï–ö–û–î–ï–†–´ –î–õ–Ø –í–´–•–û–î–ù–´–• –ú–û–î–ê–õ–¨–ù–û–°–¢–ï–ô ===
decoders:
  image:
    model: "stabilityai/stable-diffusion-2-1"
    freeze: false
  audio:
    model: "microsoft/speecht5_tts" 
    freeze: false

# === –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –ú–û–î–ê–õ–¨–ù–û–°–¢–ï–ô ===
# –î–∏—Å–∫—Ä–µ—Ç–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è non-text –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π
special_tokens:
  image_start: "<img>"
  image_end: "</img>"
  audio_start: "<aud>"
  audio_end: "</aud>"

# === –î–ê–¢–ê–°–ï–¢ ===
dataset:
  - "custom/multimodal_conversations"

conversation_field: "conversations"
max_seq_length: 4096

# === –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ===
output_dir: "./output/any2any_auto_model"
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
num_train_epochs: 2
learning_rate: 1e-5
warmup_steps: 1000

# === LoRA ===
use_peft: true
lora:
  r: 128
  alpha: 256
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
  bias: "none"

# === –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ===
gradient_checkpointing: true
bf16: true
dataloader_num_workers: 2
remove_unused_columns: false

# === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ===
logging_steps: 100
save_steps: 2000
eval_steps: 2000
report_to: "none"
run_name: "any2any_auto_training" 