# Simple multimodal training using existing models
model_name_or_path: "llava-hf/llava-1.5-7b-hf"
model_type: "standard"  # Use as standard model for now

# Dataset with conversations format
# Dataset: Convert LLaVA first using scripts/convert_llava_to_conversations.py
# dataset: ["converted_llava_instruct_150k.jsonl"]  # Path to converted dataset
dataset: ["wikitext", "wikitext-2-raw-v1"]  # Fallback for testing

# Data configuration  
conversation_field: "conversations"
max_seq_length: 1024

# Training parameters
output_dir: "./test_simple_multimodal"
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
num_train_epochs: 1
learning_rate: 1e-5

# LoRA for efficiency
use_peft: true
lora:
  r: 16
  alpha: 32
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

# Training settings
gradient_checkpointing: false
bf16: true
save_steps: 1000
logging_steps: 10
dataloader_num_workers: 1
remove_unused_columns: false
report_to: "none"

# Disable complex features for testing
generate_eval_examples: false 