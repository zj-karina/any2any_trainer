# üñºÔ∏è –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º model_type
# LLaVA-style: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ + —Ç–µ–∫—Å—Ç -> —Ç–µ–∫—Å—Ç

# === –û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ ===
model_name_or_path: "microsoft/DialoGPT-medium"

# –£–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç model_type: multimodal
modalities:
  input: ["image", "text"]
  output: ["text"]

# === VISION ENCODER ===
# –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —ç–Ω–∫–æ–¥–µ—Ä
encoders:
  image:
    model: "openai/clip-vit-large-patch14"
    freeze: true

# === –ü–†–û–ï–ö–¶–ò–Ø ===
projection:
  type: "mlp"
  hidden_size: 1024
  num_layers: 2
  dropout: 0.1

# === –î–ê–¢–ê–°–ï–¢ ===
# –¢—Ä–µ–±—É–µ—Ç—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLaVA –¥–∞—Ç–∞—Å–µ—Ç
dataset: ["llava_conversations.jsonl"]
conversation_field: "conversations"

# === –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ===
output_dir: "./output/multimodal_auto_model"
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
num_train_epochs: 3
learning_rate: 2e-5
warmup_steps: 500
max_seq_length: 2048

# === LoRA ===
use_peft: true
lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules: ["c_attn", "c_proj"]
  bias: "none"

# === –ö–û–ú–ü–û–ù–ï–ù–¢–´ ===
freeze_vision_encoder: true
freeze_llm: false
train_projection_only: false

# === –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø ===
bf16: true
dataloader_num_workers: 4
remove_unused_columns: false
generate_eval_examples: true
max_new_tokens: 256

# === –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –¢–û–ö–ï–ù–´ ===
special_tokens:
  image_start: "<img>"
  image_end: "</img>"

# === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ===
logging_steps: 50
save_steps: 1000
eval_steps: 1000
report_to: "none"
run_name: "multimodal_auto_training" 