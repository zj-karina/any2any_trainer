# Simplified configuration with direct use of HuggingFace models
# Following align-anything approach

# === BASIC PARAMETERS ===
model_name_or_path: "Qwen/Qwen2.5-1.5B-Instruct"  # Меньшая модель, которая поместится в GPU
model_type: "standard"  # Simple text model to start with

# === DATASET ===
dataset:
  - "HuggingFaceH4/ultrachat_200k"  # Используем default config

conversation_field: "messages"  # UltraChat использует поле 'messages'
max_seq_length: 2048

# === TRAINING PARAMETERS ===
output_dir: "./output/simple_hf_model"
per_device_train_batch_size: 4  # Увеличиваем, так как модель меньше
per_device_eval_batch_size: 4
gradient_accumulation_steps: 2
num_train_epochs: 1
learning_rate: 5e-5
warmup_steps: 100
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3

# === PEFT/LoRA ===
use_peft: true
lora:
  r: 32  # Уменьшаем для меньшей модели
  alpha: 64
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"

# === OPTIMIZATION ===
gradient_checkpointing: false  # Disable to avoid gradient issues
bf16: true
dataloader_num_workers: 4
remove_unused_columns: false

# === LOGGING ===
report_to: "none"
run_name: "simple_hf_training"

# === OTHER ===
seed: 42 