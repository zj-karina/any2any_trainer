# ğŸš€ Any2Any Trainer - Universal Multimodal Training Toolkit

## ğŸ“š Supported Modalities and Methods

### Modalities
- **Text:** Standard LLMs (Llama, Qwen, Mistral, etc.)
- **Images:** Vision encoders (CLIP, SigLip, ViT, etc.)
- **Audio:** Speech encoders (Whisper, WavLM, Wav2Vec2, etc.)
- **Video:** Video encoders (VideoMAE, I3D, etc.)
- **Custom:** Easily extensible architecture

### Training Methods
- **SFT (Supervised Fine-Tuning):** Standard instruction fine-tuning
- **Multimodal SFT:** Training on multimodal dialogues
- **LoRA/QLoRA:** Efficient fine-tuning with low-rank adapters
- **Full Fine-tuning:** Complete parameter fine-tuning
- **Projection Training:** Training only projection layers

### Architectures
- **Encoder-Decoder:** Separate encoders for each modality + shared decoder
- **Unified:** Single model for all modalities (like AnyGPT)
- **Projection-based:** LLM + projection adapters (like LLaVA)

## ğŸ§  Smart Configuration

### ğŸš€ NEW: Automatic Model Type Detection

Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ **ĞĞ• ĞĞ£Ğ–ĞĞ** ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ `model_type` Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸! Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ Ñ‚Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹:

```yaml
# ğŸ¤– ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "standard"
modalities:
  input: ["text"]
  output: ["text"]

# ğŸ¤– ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "multimodal"  
modalities:
  input: ["image", "text"]
  output: ["text"]

# ğŸ¤– ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "any2any"
modalities:
  input: ["text", "image", "audio"]
  output: ["text", "image", "audio"]
```

## ğŸš€ How to Use

### ğŸ“¦ Installation

**Quick Setup:**
```bash
cd any2any_trainer
poetry install
export HF_HOME=/mnt/hf/
```

## ğŸš€ Quick Start

```bash
# Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹)
poetry run python scripts/train_multimodal.py configs/sft/minimal_working.yaml

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
poetry run python test_installation.py  # Confirms: Model loading, LoRA, CUDA, forward pass
```

## ğŸ—ï¸ Library Architecture

```
any2any_trainer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # Model architectures
â”‚   â”‚   â”œâ”€â”€ encoders/     # Encoders for different modalities
â”‚   â”‚   â”œâ”€â”€ decoders/     # Decoders for different modalities
â”‚   â”‚   â”œâ”€â”€ projectors/   # Projection layers
â”‚   â”‚   â””â”€â”€ unified/      # Unified models (AnyGPT-style)
â”‚   â”œâ”€â”€ data/            # Data processing
â”‚   â”‚   â”œâ”€â”€ datasets/    # Datasets for different modalities
â”‚   â”‚   â”œâ”€â”€ collators/   # Data collators
â”‚   â”‚   â””â”€â”€ tokenizers/  # Modality tokenizers
â”‚   â”œâ”€â”€ training/        # Training
â”‚   â”‚   â”œâ”€â”€ trainers/    # Custom trainers
â”‚   â”‚   â”œâ”€â”€ losses/      # Loss functions
â”‚   â”‚   â””â”€â”€ callbacks/   # Training callbacks
â”‚   â””â”€â”€ utils/           # Utilities
â”œâ”€â”€ scripts/             # Launch scripts
â”œâ”€â”€ configs/             # YAML configurations
â”œâ”€â”€ accelerate/          # Accelerate configs
â””â”€â”€ deepspeed_configs/   # DeepSpeed configs
```

## ğŸ“Š Data Format

Any2Any Trainer uses **single standard format** based on OpenAI-style conversations:

```json
{
  "conversations": [
    {
      "role": "user",
      "content": "What's in the image?",
      "image": "path/to/image.jpg"  // optional multimodal data
    },
    {
      "role": "assistant", 
      "content": "The image shows a cat sitting on a window."
    }
  ]
}
```


```json
{
  "conversations": [
    {
      "role": "user", 
      "content": "What is machine learning?"
    },
    {
      "role": "assistant",
      "content": "Machine learning is a subset of artificial intelligence..."
    }
  ]
}
```

## ğŸ¯ Development Roadmap

- [x] Direct HuggingFace model usage
- [x] Configuration system and modular architecture
- [ ] Complete implementation of trainers and datasets
- [ ] Video modality support
- [ ] Integration with Gemini, GPT-4V API
- [ ] 3D model support
- [ ] Web interface for configuration creation
- [ ] Automatic hyperparameter tuning

## ğŸ¤ How to Contribute

We welcome contributions to the library development! Main areas:
- Adding new encoders/decoders
- Supporting new modalities
- Performance optimization
- Documentation improvements

## ğŸ“„ License

Apache-2.0 License 