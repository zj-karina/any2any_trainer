# ğŸš€ Any2Any Trainer - Universal Multimodal Training Toolkit

> **New here?** Jump to [Installation](#-installation) â†’ [Quick Start](#-quick-start) â†’ [Examples](#-example-yaml-configurations)

## Table of Contents
- [âœ¨ Features](#-description-and-features)
- [ğŸ“¦ Installation](#-installation) 
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ Configuration Examples](#-example-yaml-configurations)
- [ğŸ§  Smart Configuration](#-smart-configuration)
- [ğŸ“š Documentation](#-documentation)
- [ğŸ¯ Roadmap](#-development-roadmap)

## âœ¨ Description and Features

This is a **universal**, **customizable**, **user-friendly**, and **efficient** toolkit for training any-to-any multimodal models. 

Simply define a YAML configuration with HF TrainingArguments parameters and specific parameters for each modality.

### ğŸ› ï¸ Toolkit Foundation

**Core Libraries:**
- **Core:** PyTorch, Transformers, TRL
- **Distributed Training:** Accelerate, FSDP, DeepSpeed (Zero 2/3)
- **Acceleration:** vLLM, Flash Attention, SDPA
- **Build and Installation:** Poetry
- **Result Logging:** wandb, clearml, tensorboard

## ğŸ“š Supported Modalities and Methods

### Modalities
- **Text:** Standard LLMs (Llama, Qwen, Mistral, etc.)
- **Images:** Vision encoders (CLIP, SigLip, ViT, etc.)
- **Audio:** Speech encoders (Whisper, WavLM, Wav2Vec2, etc.)
- **Video:** Video encoders (VideoMAE, I3D, etc.)
- **Custom:** Easily extensible architecture

### Training Methods
- **SFT (Supervised Fine-Tuning):** Standard instruction fine-tuning
- **Multimodal SFT:** Training on multimodal dialogues
- **LoRA/QLoRA:** Efficient fine-tuning with low-rank adapters
- **Full Fine-tuning:** Complete parameter fine-tuning
- **Projection Training:** Training only projection layers

### Architectures
- **Encoder-Decoder:** Separate encoders for each modality + shared decoder
- **Unified:** Single model for all modalities (like AnyGPT)
- **Projection-based:** LLM + projection adapters (like LLaVA)

## ğŸ§  Smart Configuration

### ğŸš€ NEW: Automatic Model Type Detection

Ğ‘Ğ¾Ğ»ÑŒÑˆĞµ **ĞĞ• ĞĞ£Ğ–ĞĞ** ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ñ‚ÑŒ `model_type` Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸! Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ Ñ‚Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹:

```yaml
# ğŸ¤– ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "standard"
modalities:
  input: ["text"]
  output: ["text"]

# ğŸ¤– ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "multimodal"  
modalities:
  input: ["image", "text"]
  output: ["text"]

# ğŸ¤– ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "any2any"
modalities:
  input: ["text", "image", "audio"]
  output: ["text", "image", "audio"]
```

### Ğ›Ğ¾Ğ³Ğ¸ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ:
- **`standard`**: `text â†’ text` Ğ¸Ğ»Ğ¸ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, `image â†’ image`)
- **`multimodal`**: ĞĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ² Ñ‚ĞµĞºÑÑ‚
- **`any2any`**: Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹

## ğŸš€ How to Use

### ğŸ“¦ Installation

**Quick Setup:**
```bash
cd any2any_trainer
poetry install

## ğŸš€ Quick Start

### âœ… Test Installation (VERIFIED WORKING)
```bash
poetry run python test_installation.py
```

### âœ… Ğ Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ (ĞŸĞ ĞĞ’Ğ•Ğ Ğ•ĞĞĞ«Ğ•)

| ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ | ĞœĞ¾Ğ´ĞµĞ»ÑŒ | Ğ¢Ğ¸Ğ¿ | Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ |
|--------------|--------|-----|--------|
| `minimal_working.yaml` | GPT-2 | standard | âœ… Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ |
| `simple_hf_training.yaml` | Qwen-7B | standard | âœ… Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ |
| `llava_training.yaml` | DialoGPT + CLIP | multimodal | âœ… Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ |

```bash
# Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Ñ‚ĞµÑÑ‚ (ÑĞ°Ğ¼Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹)
poetry run python scripts/train_multimodal.py configs/sft/minimal_working.yaml

# ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµÑÑ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
poetry run python test_installation.py  # Confirms: Model loading, LoRA, CUDA, forward pass
```

### Training Configuration Example
```bash
# Create simple config - model_type Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "standard"!
echo 'model_name_or_path: "gpt2"
modalities:
  input: ["text"]
  output: ["text"]
dataset: ["wikitext", "wikitext-2-raw-v1"]
output_dir: "./my_first_model"
per_device_train_batch_size: 1
num_train_epochs: 1
use_peft: true
lora:
  r: 8
  alpha: 16
  target_modules: ["c_attn"]' > quick_test.yaml
```

### ğŸƒâ€â™‚ï¸ Example Training Commands

```bash



# ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ HuggingFace Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (Qwen + LoRA)  
poetry run python scripts/train_multimodal.py configs/sft/simple_hf_training.yaml

# ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ (LLaVA-style)
poetry run python scripts/train_multimodal.py configs/sft/llava_training.yaml
```

### ğŸ¤— Simple HuggingFace Model Usage

#### Minimal Configuration

```yaml
# Just specify any model from HF Hub! model_type Ğ°Ğ²Ñ‚Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "standard"
model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
modalities:
  input: ["text"]
  output: ["text"]
dataset: ["tatsu-lab/alpaca"]
use_peft: true
lora:
  target_modules: ["q_proj", "v_proj"]
per_device_train_batch_size: 2
output_dir: "./my_model"
```

**Run:**
```bash
python scripts/train_multimodal.py my_config.yaml
```

### ğŸ“ Example YAML Configurations

#### LLaVA-style Model Training (image â†’ text)

```yaml
# configs/sft/llava_style_training.yaml
model_name_or_path: "microsoft/DialoGPT-medium"
# model_type Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "multimodal"!

# IMPORTANT: Convert LLaVA dataset first!
# Use: python scripts/convert_llava_to_conversations.py llava_data.jsonl converted_data.jsonl
dataset:
  - "converted_llava_data.jsonl"  # Path to converted LLaVA dataset
  
modalities:
  input: ["image", "text"]
  output: ["text"]

encoders:
  image:
    model: "openai/clip-vit-base-patch32"
    freeze: true

projection:
  type: "mlp"  # mlp, linear, transformer

per_device_train_batch_size: 2
per_device_eval_batch_size: 2
num_train_epochs: 3
learning_rate: 2e-5
gradient_accumulation_steps: 4
max_seq_length: 2048

# Specific parameters for multimodal training
freeze_vision_encoder: true
freeze_llm: false
train_projection_only: false

use_peft: true
lora_r: 64
lora_alpha: 128
lora_target_modules:
  - "q_proj"
  - "v_proj"
  - "k_proj"
  - "o_proj"

conversation_field: "conversations"
image_field: "image"
```

#### Any-to-Any Model Training (any modality â†’ any modality)

```yaml
# configs/any2any/anygpt_style_training.yaml
model_name_or_path: "Qwen/Qwen2.5-7B-Instruct"
# model_type Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑÑ ĞºĞ°Ğº "any2any"!

modalities:
  input: ["text", "image", "audio", "video"]
  output: ["text", "image", "audio"]

# Encoders for each modality
encoders:
  image:
    model: "openai/clip-vit-large-patch14"
    freeze: true
    tokenizer_type: "discrete"
  audio:
    model: "openai/whisper-base"
    freeze: true
    tokenizer_type: "discrete"
  video:
    model: "microsoft/videoMAE-base"
    freeze: true
    tokenizer_type: "discrete"

# Decoders for output modalities
decoders:
  image:
    model: "stabilityai/stable-diffusion-2-1"
    freeze: false
  audio:
    model: "microsoft/speecht5_tts"
    freeze: false

# Special tokens for modalities
special_tokens:
  image_start: "<img>"
  image_end: "</img>"
  audio_start: "<aud>"
  audio_end: "</aud>"
  video_start: "<vid>"
  video_end: "</vid>"

dataset:
  - "custom/multimodal_conversations"

per_device_train_batch_size: 1
gradient_accumulation_steps: 16
num_train_epochs: 2
learning_rate: 1e-5
max_seq_length: 4096

use_peft: true
lora_r: 128
```

## ğŸ—ï¸ Library Architecture

```
any2any_trainer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # Model architectures
â”‚   â”‚   â”œâ”€â”€ encoders/     # Encoders for different modalities
â”‚   â”‚   â”œâ”€â”€ decoders/     # Decoders for different modalities
â”‚   â”‚   â”œâ”€â”€ projectors/   # Projection layers
â”‚   â”‚   â””â”€â”€ unified/      # Unified models (AnyGPT-style)
â”‚   â”œâ”€â”€ data/            # Data processing
â”‚   â”‚   â”œâ”€â”€ datasets/    # Datasets for different modalities
â”‚   â”‚   â”œâ”€â”€ collators/   # Data collators
â”‚   â”‚   â””â”€â”€ tokenizers/  # Modality tokenizers
â”‚   â”œâ”€â”€ training/        # Training
â”‚   â”‚   â”œâ”€â”€ trainers/    # Custom trainers
â”‚   â”‚   â”œâ”€â”€ losses/      # Loss functions
â”‚   â”‚   â””â”€â”€ callbacks/   # Training callbacks
â”‚   â””â”€â”€ utils/           # Utilities
â”œâ”€â”€ scripts/             # Launch scripts
â”œâ”€â”€ configs/             # YAML configurations
â”œâ”€â”€ accelerate/          # Accelerate configs
â””â”€â”€ deepspeed_configs/   # DeepSpeed configs
```

## ğŸ“Š Data Format

**ğŸ“‹ [READ DATA_FORMAT.md](DATA_FORMAT.md) - Complete specification**

Any2Any Trainer uses **single standard format** based on OpenAI-style conversations:

```json
{
  "conversations": [
    {
      "role": "user",
      "content": "What's in the image?",
      "image": "path/to/image.jpg"  // optional multimodal data
    },
    {
      "role": "assistant", 
      "content": "The image shows a cat sitting on a window."
    }
  ]
}
```

**âš ï¸ Important**: We do NOT provide automatic format conversion. Convert your data to this standard format before training.

**âœ… Compatible with**: OpenAI API, HuggingFace chat templates, TRL, effective_llm_alignment

**ğŸ”§ Format Converters Available:**
- **LLaVA Dataset**: 
  - `scripts/convert_llava_to_conversations.py` - converts LLaVA format to conversations format
  - `scripts/download_and_convert_llava.py` - downloads and converts LLaVA-Instruct-150K automatically

**ğŸ“¥ Quick LLaVA Setup:**
```bash
# Option 1: Automatic download and conversion
poetry run python scripts/download_and_convert_llava.py

# Option 2: Manual conversion (if you have LLaVA data)
poetry run python scripts/convert_llava_to_conversations.py input.jsonl output.jsonl

# Then train with:
python scripts/train_multimodal.py configs/sft/llava_training.yaml
```

### âŒ Model Type Errors
Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ Ñ‡Ñ‚Ğ¾ `model_type` Ğ² YAML Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ·:
- `"standard"` - Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğµ HuggingFace Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
- `"multimodal"` - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (LLaVA-style) 
- `"any2any"` - any-to-any Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸

## ğŸ“š Documentation

### ğŸ“– Detailed Guides
- ğŸ¤— [**HF_MODELS_USAGE.md**](HF_MODELS_USAGE.md) - Complete guide for using HuggingFace models

### ğŸ“‹ Quick Reference
- [Installation](#-installation)
- [Quick Start](#-quick-start) 
- [Configuration Examples](#-example-yaml-configurations)
- [Supported Models](#-supported-modalities-and-methods)

## ğŸ¯ Development Roadmap

- [x] Direct HuggingFace model usage
- [x] Configuration system and modular architecture
- [ ] Complete implementation of trainers and datasets
- [ ] Video modality support
- [ ] Integration with Gemini, GPT-4V API
- [ ] 3D model support
- [ ] Web interface for configuration creation
- [ ] Automatic hyperparameter tuning

## ğŸ¤ How to Contribute

We welcome contributions to the library development! Main areas:
- Adding new encoders/decoders
- Supporting new modalities
- Performance optimization
- Documentation improvements

## ğŸ“„ License

Apache-2.0 License 